{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier for non-linearly separable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mP6nNNTVNGhz"
   },
   "outputs": [],
   "source": [
    "# Import usual libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make all figures tiny for readability purpose\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = (5,3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Let's build our \"XOR\" dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to classify a non-linearly separable dataset like this one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/05-Machine-Learning/05-Model-Tuning/non-linear-dataset.png\" width=450>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To that end, we will create a 2D dataset using [logical XOR gates](https://en.wikipedia.org/wiki/XOR_gate)\n",
    "\n",
    "Try to understand how this works by playing with `np.logical_xor` in the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.logical_xor(True, True))\n",
    "print(np.logical_xor(True, False))\n",
    "print(np.logical_xor(False, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logical_xor([True, True, False], [True, False, False])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create our own synthetic 2D dataset where: \n",
    "\n",
    "$$ y = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if } (X_1>0 \\textbf{ xor } X_2>0) \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of datapoints\n",
    "n = 500\n",
    "noise = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "X = rng.standard_normal((n, 2))\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "x1 = X[:, 0]\n",
    "x2 = X[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, build our binary class y\n",
    "# where y=1 if and only if (X1 > Œµ xor X2 > Œµ)\n",
    "# epsilon Œµ not exactly zero to simulate \"noise\"\n",
    "\n",
    "epsilon = noise * rng.standard_normal(n)\n",
    "\n",
    "y_bool = np.logical_xor(\n",
    "    x1 > epsilon,\n",
    "    x2 > epsilon\n",
    ")\n",
    "y = np.where(y_bool, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have re-created the XOR quadrant!\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Linear SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome _Support Vector Machine (SVM)_ üéâ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Linear SVM classifier's goal is to find the best vector **w**\n",
    "\n",
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/05-Machine-Learning/05-Model-Tuning/hyperplane.png\" width=500>\n",
    "\n",
    "- whose direction uniquely determines the decision boundary hyperplane\n",
    "- and minimizes the sum of hinge losses for outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "üëâ **w** contains all model parameters learned during `.fit()`  \n",
    "üëâ `C` is the **cost** associated with the **wrong** classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "‚ùì Fit a **linear** SVC classifier on the whole dataset with the value `C` = 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 783
    },
    "colab_type": "code",
    "id": "01tws6QLNLo4",
    "outputId": "d4a81201-7391-4d94-dff8-e7e85b92e658",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Visualize the decision boundaries of your model using the `plot_decision_regions` function given to you in `utils/plots.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 783
    },
    "colab_type": "code",
    "id": "01tws6QLNLo4",
    "outputId": "d4a81201-7391-4d94-dff8-e7e85b92e658"
   },
   "outputs": [],
   "source": [
    "from utils.plots import plot_decision_regions\n",
    "\n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìSave below your \"guesstimated\" accuracy score for this model (roughly) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_svm_score = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Do you think the value of `C` has any importance in this case?\n",
    "\n",
    "<details><summary>Answer</summary>\n",
    "    \n",
    "‚òùÔ∏è Linear SVM classifiers always separate data points with a straight line, no matter the cost, `c`, being applied to wrongly classified data points. We are structurally trying to fit the wrong type of model to the dataset.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Do you think a Logistic Regression would perform better? Feel free to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('linear_svm',\n",
    "    linear_svm_score=linear_svm_score)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kernel SVM üî•"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö First, **read section 5 (Kernels) of today's lecture carefully**. Take your time, it is part of the challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìIn your own words, what is a Kernel in one sentence?\n",
    "\n",
    "<details>\n",
    "    <summary>A possible answer</summary>\n",
    "\n",
    "It is a measure of \"similarity\" between points, which is used to classify points in SVM models (two points with large similarity would be classified similarly)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìCite below 4 different kernel by their names\n",
    "\n",
    "<details>\n",
    "    <summary>A possible answer</summary>\n",
    "\n",
    "- Linear\n",
    "- Polynomial\n",
    "- Radial Basis Fonction (rbf)\n",
    "- Sigmoid\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Polynomial Kernels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A polynomial kernel of degree 2 is as follows \n",
    "    $$K(\\textbf{a},\\textbf{b}) = (\\textbf{a}^T\\textbf{b} + c )^2$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the kernel, we have changed our notion of _similarity_: instead of measuring similarity by how close the points are in terms of dot product (\"cosine similarity\") for the linear kernel, we are measuring similarity based on **whether points are within a circle to each other or not.** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is **equivalent** to creating the new quadratic features as below (Kernel Trick)\n",
    "    $$\\phi(x) = \\begin{bmatrix} x_1^2 \\\\ x_1 x_2 \\\\ x_2 x_1 \\\\ x_2^2 \\\\  \\sqrt{2c} \\ x_1 \\\\ \\sqrt{2c} \\ x_2\\end{bmatrix}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Intuitively speaking, what do you think is the minimum number of kernel \"degrees\" `d` we need to best fit our XOR dataset? \n",
    "\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "`d=2` should be enough because our XOR condition can be computed from polynom `x1 * x2`\n",
    "    \n",
    "$$    \n",
    "    class = \n",
    "\\begin{cases}\n",
    "    0 & \\text{if } x_1 x_2 > 0\\\\\n",
    "    1 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Fit and plot a SVM `poly` of degree 2, keeping `C = 100` large enough so as to visualize the \"maximum margin classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Given that you know how this XOR dataset has been engineered, which adjectives best describe your svm's performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"is underfitting\"\n",
    "b = \"captures all reducible error\"\n",
    "c = \"has too high variance\"\n",
    "d = \"has only irreducible errors left\"\n",
    "\n",
    "poly_svm_performance = None # [?] # fill the list with the answer(s)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('poly_svm',\n",
    "    poly_svm_performance=poly_svm_performance)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Still not convinced? You can double-check that the polynomial kernel is equivalent to having access to polynomial features by fitting this model below:\n",
    "\n",
    "- We create polynomial features as a first step (feature engineering)\n",
    "- and then fit a linear SVM.\n",
    "\n",
    "(And don't worry about the syntax of pipelines, we will see more on pipelines in the next lecture! üî•)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Choose your degree\n",
    "degree = None\n",
    "\n",
    "equivalent_polynomial_model = make_pipeline(\n",
    "    (PolynomialFeatures(degree=degree)),  # First, create all polynomial combinations of your features\n",
    "    (SVC(kernel=\"linear\", C=100))  # Then, fit a linear SVM\n",
    ")\n",
    "equivalent_polynomial_model.fit(X, y)\n",
    "plot_decision_regions(X, y, classifier=equivalent_polynomial_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Although mathematically equivalent, this manual feature engineering requires much more computational power than the kernel trick, and will not scale with higher dimensionality!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moons dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try with a harder dataset to classifiy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "n=200\n",
    "X,y = make_moons(n_samples=n, noise=0.25, random_state=0)\n",
    "plt.scatter(X[:,0], X[:,1], c=y);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Try to fit a polynomial SVM kernel to this dataset \n",
    "\n",
    "Try it out **visually** below by changing the values of `C` and `degree` and `coef0`.\n",
    "\n",
    "`coef0` plays the role of $c$ in $K(\\textbf{a},\\textbf{b}) = (\\textbf{a}^T\\textbf{b} + c )^d$\n",
    "- When equal to 0, you only have access to the d-th degree polynomial features\n",
    "- The higher it is, the more your model will consider lower degree features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact # pip install ipywidgets if you haven't done it already\n",
    "\n",
    "@interact(C=[1, 10, 1000, 10000, 100000], degree=[1,2,3,4,5,6,7,8,9], coef0=[0,0.5,1,2,5,10,100])\n",
    "def svc(C, degree, coef0):\n",
    "    svm = SVC(kernel='poly', C=C, coef0=coef0, degree=degree)\n",
    "    svm.fit(X, y)\n",
    "    plot_decision_regions(X, y, classifier=svm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è A polynomial kernel can fit pretty much anything as long as the degree is high enough.  \n",
    "\n",
    "However, be aware that polynomial kernels with too high a degree will make models prone to overfitting!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 RBF - Radial Basis Function Kernel (aka gaussian)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the RBF Kernel! \n",
    "Pretty much the go-to kernel for SVM (and default one in sklearn).   \n",
    "\n",
    "It proves very robust to a variety of problems, and is easier to fine-tune than polynomial, as it only requires gridsearching its kernel hyper-parameter `gamma` $\\gamma$, on top of `C` of course"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K(\\textbf{a},\\textbf{b}) = \\exp[ \\left( - \\gamma ||\\textbf{a}-\\textbf{b}||^2\\right)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The larger the euclidian distance between two points $||\\textbf{a}-\\textbf{b}||^2$, the closer the kernel function is to zero. This means that two points far away are more likely to be different.\n",
    "\n",
    "- `gamma` $\\gamma$ reduces each instance‚Äôs range of influence on the others (**myopia**). Stronger $\\gamma$ values will make your model overfit by looking \"too closely\" at small-scale irregularities\n",
    "\n",
    "- `C` still controls the **hardness** of the margin. Stronger values will make your model overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's show below the impact of gamma \"myopia\" in the Kernel\n",
    "for gamma in np.linspace(0.1, 1, 4):\n",
    "    plt.plot(np.exp(-gamma*np.linspace(0, 3, 100)**2),\n",
    "             label=f'gamma = {gamma}')\n",
    "plt.ylim(0,)\n",
    "plt.title('RBF Kernel')\n",
    "plt.xlabel('euclidian distance between points')\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Fit and plot decision regions of an SVM classifier with an RBF kernel\n",
    "\n",
    "- Start with a model with very low variance (ex: `gamma=0.01` and `C=0.1`)\n",
    "- Increase `gamma` until you observe obvious overfitting. It should be clearly visible on the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Now, use you best estimated value of gamma (for instance `gamma=1`), and increase `C` this time until you overfit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Feel free to create your own interactive plot using `@interact` as above and try to \"visually\" find the best `C` and `gamma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 810,
     "referenced_widgets": [
      "b60b33ead3e045918812744e23f2dac2",
      "5d4516d80e3b4a90b7411e8a81df6d89",
      "d0bb503cf16c463abe504958f9afd476",
      "fac58c96897347929b40dcbf4089d867",
      "3ca6720467454a3b819324c7428930c8",
      "7aabc06a42704880afe3437daf35ac5a",
      "b275f8e448e44caf9a86919d0f3102d8"
     ]
    },
    "colab_type": "code",
    "id": "0F3RJYBZN2uV",
    "outputId": "534da6eb-68f1-4cd0-f921-969c154f548a"
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "@interact(C=[0.1, 1, 10, 100, 1000, 10000], gamma = [0.001, 0.01, 0.1, 1, 10])\n",
    "def svc(C=1, gamma=1):\n",
    "    svm = SVC(kernel='rbf', gamma=gamma, C=C)\n",
    "    svm.fit(X, y)\n",
    "    plot_decision_regions(X, y, classifier=svm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìStore below your best visual guess for `C` and `gamma` (rounded to powers of ten: 0.1, 1, 10, 100 ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_c = None #?\n",
    "best_gamma = None #? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('rbf_svm',\n",
    "                         best_c=best_c, \n",
    "                         best_gamma=best_gamma)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Sigmoid kernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K(\\textbf{a},\\textbf{b}) = tanh(\\gamma \\textbf{a}^T \\textbf{b} +r)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"tangent hyperbolic\" function resembles sigmoid but can take negative values\n",
    "x = np.linspace(-3,3,100)\n",
    "plt.plot(x, np.tanh(x))\n",
    "plt.xlabel('cosine distance'); plt.title('Sigmoid Kernel');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è The Sigmoid kernel compresses the \"cosine similarity\" used in the linear kernel into a [-$\\gamma$,+$\\gamma$] interval."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Try to **visually** fine tune your model with the sigmoid kernel, doing a coarse grain search for \n",
    "- `C=[0.1, 1, 10, 100, 1000, 10000]` \n",
    "- `gamma = [0.001, 0.01, 0.1, 1, 10]`  \n",
    "\n",
    "using the `@interact` decorator combined with the `plot_decision_regions` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(C=[0.1, 1, 10, 100, 1000, 10000], gamma = [0.001, 0.01, 0.1, 1, 10], coef0=0.)\n",
    "def svc(C=1000, gamma=0.1, coef0=0):\n",
    "    svm = SVC(kernel='sigmoid', gamma=gamma, C=C, coef0=coef0)\n",
    "    svm.fit(X, y)\n",
    "    plot_decision_regions(X, y, classifier=svm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è  Not easy to make it work visually, right? Time for an automated GridSearch!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Grid Search the best kernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì `RandomizedSearchCV` for the best `kernel`, and best kernel hyperparameters at the same time (warning: scikit-learn has issues when gridsearching polynomial kernels at the same time as others)\n",
    " \n",
    "Use your visual intuitions above to define plausible ranges of values to try out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Plot your best model's decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì 5-fold cross-validate its accuracy and store your mean result as `best_svm_cv_accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('sigmoid_svm',\n",
    "    best_svm_cv_accuracy=best_svm_cv_accuracy)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìChoose your `best_svm` model to put in \"production\" in your app on the cloud. Fit it on the half moon dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few days, the model has received and predicted new data points never having seen them before (our \"test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a half-moon test_set of size n_test\n",
    "n_test = 100\n",
    "X_test, y_test = make_moons(n_samples=n, noise=0.2, random_state=1)\n",
    "\n",
    "X_full = np.vstack((X, X_test))\n",
    "y_full = np.append(y, values = y_test)\n",
    "test_idx = np.arange(n,n+n_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Visualize its performance by doing `plot_decision_regions` including `test_idx` as optional argument.  \n",
    "Count how many misclassified test samples you get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_misclassified_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('generalization',\n",
    "    number_misclassified_test=number_misclassified_test)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YeRSiecQayiP"
   },
   "source": [
    "## 6. kNN vs. SVM ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such non-linear problem could also fit well on a KNN classifier.\n",
    "Try to find the best K visually and compare its behavior to the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Congratulation! Dont forget to **commit** and **push** your notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
